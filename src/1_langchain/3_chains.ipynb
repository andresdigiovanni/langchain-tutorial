{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef00009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda, RunnableParallel\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fbbb56",
   "metadata": {},
   "source": [
    "# 1. Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74730647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model integrated with LangChain\n",
    "task = \"text2text-generation\"\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    "hf_pipeline = pipeline(task, model=model_name, max_new_tokens=50)\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83439cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt templates (no need for separate Runnable chains)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff5ab5",
   "metadata": {},
   "source": [
    "# 2. Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403efea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define additional processing steps using RunnableLambda\n",
    "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
    "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\\n{x}\")\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | llm | StrOutputParser() | uppercase_output | count_words\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2056ef3",
   "metadata": {},
   "source": [
    "# 3. Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert product reviewer.\"),\n",
    "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define pros analysis step\n",
    "def analyze_pros(features):\n",
    "    pros_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert product reviewer.\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features: {features}, list the pros of these features.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return pros_template.format_prompt(features=features)\n",
    "\n",
    "\n",
    "# Define cons analysis step\n",
    "def analyze_cons(features):\n",
    "    cons_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert product reviewer.\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features: {features}, list the cons of these features.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return cons_template.format_prompt(features=features)\n",
    "\n",
    "\n",
    "# Combine pros and cons into a final review\n",
    "def combine_pros_cons(pros, cons):\n",
    "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
    "\n",
    "\n",
    "# Simplify branches with LCEL\n",
    "pros_branch_chain = RunnableLambda(lambda x: analyze_pros(x)) | llm | StrOutputParser()\n",
    "\n",
    "cons_branch_chain = RunnableLambda(lambda x: analyze_cons(x)) | llm | StrOutputParser()\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = (\n",
    "    prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
    "    | RunnableLambda(\n",
    "        lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe22ea8",
   "metadata": {},
   "source": [
    "# 4. Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe846d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt templates for different feedback types\n",
    "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the feedback classification template\n",
    "classification_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the runnable branches for handling feedback\n",
    "branches = RunnableBranch(\n",
    "    (\n",
    "        lambda x: \"positive\" in x,\n",
    "        positive_feedback_template | llm | StrOutputParser(),  # Positive feedback chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"negative\" in x,\n",
    "        negative_feedback_template | llm | StrOutputParser(),  # Negative feedback chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"neutral\" in x,\n",
    "        neutral_feedback_template | llm | StrOutputParser(),  # Neutral feedback chain\n",
    "    ),\n",
    "    escalate_feedback_template | llm | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# Create the classification chain\n",
    "classification_chain = classification_template | llm | StrOutputParser()\n",
    "\n",
    "# Combine classification and response generation into one chain\n",
    "chain = classification_chain | branches\n",
    "\n",
    "# Run the chain with an example review\n",
    "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
    "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
    "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
    "\n",
    "review = (\n",
    "    \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    ")\n",
    "result = chain.invoke({\"feedback\": review})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
