{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95160779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455f74f",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a79685",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a068a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x**y\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x\n",
    "\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate]\n",
    "name2tool = {t.name: t.func for t in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deefd602",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You're a helpful assistant. When answering a user's question, \"\n",
    "                \"you may use one of the provided tools if needed. After using a \"\n",
    "                \"tool, you can provide a final answer directly. \"\n",
    "                \"DO NOT use the same tool more than once.\"\n",
    "            ),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99712be5",
   "metadata": {},
   "source": [
    "# 2. Streaming with astream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52376114",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "async for token in llm.astream(\"What is NLP?\"):\n",
    "    tokens.append(token)\n",
    "    print(token.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17635988",
   "metadata": {},
   "source": [
    "# 3. Queue Callback Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", []),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueueCallbackHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler that puts tokens into a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, queue: asyncio.Queue):\n",
    "        self.queue = queue\n",
    "\n",
    "    async def __aiter__(self):\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                await asyncio.sleep(0.1)\n",
    "                continue\n",
    "\n",
    "            token_or_done = await self.queue.get()\n",
    "            if token_or_done == \"<<DONE>>\":\n",
    "                return\n",
    "            if token_or_done:\n",
    "                yield token_or_done\n",
    "\n",
    "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
    "        chunk = kwargs.get(\"chunk\")\n",
    "        if chunk:\n",
    "            self.queue.put_nowait(chunk)\n",
    "\n",
    "    async def on_llm_end(self, *args, **kwargs) -> None:\n",
    "        self.queue.put_nowait(\"<<DONE>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644126e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "\n",
    "async def stream(query: str):\n",
    "    response = agent.with_config(callbacks=[streamer])\n",
    "    async for token in response.astream(\n",
    "        {\"input\": query, \"chat_history\": [], \"agent_scratchpad\": []}\n",
    "    ):\n",
    "        tokens.append(token)\n",
    "        print(token, flush=True)\n",
    "\n",
    "\n",
    "await stream(\"What is 10 + 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9284494e",
   "metadata": {},
   "source": [
    "# 4. Custom Agent Executor - astream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", []),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools)\n",
    "        )\n",
    "\n",
    "    async def invoke(\n",
    "        self, input: str, streamer: QueueCallbackHandler, verbose: bool = False\n",
    "    ) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        final_output = None\n",
    "\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            async def stream(query: str):\n",
    "                response = self.agent.with_config(callbacks=[streamer])\n",
    "                # we initialize the output dictionary that we will be populating with\n",
    "                # our streamed output\n",
    "                output = None\n",
    "                # now we begin streaming\n",
    "                async for token in response.astream(\n",
    "                    {\n",
    "                        \"input\": query,\n",
    "                        \"chat_history\": self.chat_history,\n",
    "                        \"agent_scratchpad\": agent_scratchpad,\n",
    "                    }\n",
    "                ):\n",
    "                    if output is None:\n",
    "                        output = token\n",
    "                    else:\n",
    "                        # we can just add the tokens together as they are streamed and\n",
    "                        # we'll have the full response object at the end\n",
    "                        output += token\n",
    "\n",
    "                    if verbose and token.content:\n",
    "                        # we can capture various parts of the response object\n",
    "                        print(f\"content: {token.content}\", flush=True)\n",
    "\n",
    "                    tool_calls = token.additional_kwargs.get(\"tool_calls\")\n",
    "                    if tool_calls and verbose:\n",
    "                        print(f\"tool_calls: {tool_calls}\", flush=True)\n",
    "\n",
    "                return output\n",
    "\n",
    "            output = await stream(query=input)\n",
    "\n",
    "            if not output or not getattr(output, \"tool_calls\", None):\n",
    "                print(\"Detected final answer.\")\n",
    "                final_output = (\n",
    "                    output.content if hasattr(output, \"content\") else str(output)\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # Parse tool call\n",
    "            tool_call = output.tool_calls[0]\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_args = (\n",
    "                json.loads(tool_args) if isinstance(tool_args, str) else tool_args\n",
    "            )\n",
    "            tool_call_id = tool_call[\"id\"]\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{count}: {tool_name}({tool_args})\")\n",
    "\n",
    "            tool_out = name2tool[tool_name](**tool_args)\n",
    "\n",
    "            # Append tool call + result\n",
    "            agent_scratchpad.append(\n",
    "                AIMessage(\n",
    "                    content=output.content,\n",
    "                    tool_calls=output.tool_calls,\n",
    "                )\n",
    "            )\n",
    "            agent_scratchpad.append(\n",
    "                ToolMessage(content=f\"{tool_out}\", tool_call_id=tool_call_id)\n",
    "            )\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        # Add exchange to chat history\n",
    "        self.chat_history.extend(\n",
    "            [HumanMessage(content=input), AIMessage(content=final_output)]\n",
    "        )\n",
    "\n",
    "        return {\"answer\": final_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bef56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = CustomAgentExecutor()\n",
    "\n",
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "out = await agent_executor.invoke(\"What is 10 + 10\", streamer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda70c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
