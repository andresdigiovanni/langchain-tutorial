{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55643f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.messages import BaseMessage, SystemMessage\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24509f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"deepseek-r1:8b\", reasoning=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a289f4",
   "metadata": {},
   "source": [
    "# 1. Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572361e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eee7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e7cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# Invoke\n",
    "msgs = [\"Hi, my name is James\", \"What is my name again?\"]\n",
    "for i, msg in enumerate(msgs):\n",
    "    print(f\"---\\nMessage {i + 1}\\n---\\n\")\n",
    "    response = pipeline_with_history.invoke(\n",
    "        {\"query\": msg}, config={\"session_id\": \"id_123\"}\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4283b2",
   "metadata": {},
   "source": [
    "# 2. Message History + Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond the last `k` messages.\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k :]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0cf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Invoke\n",
    "msgs = [\n",
    "    \"Hi, my name is James\",\n",
    "    \"I like go to the swimming pool.\",\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"What is my name again?\",\n",
    "]\n",
    "for i, msg in enumerate(msgs):\n",
    "    print(f\"---\\nMessage {i + 1}\\n---\\n\")\n",
    "    response = pipeline_with_history.invoke(\n",
    "        {\"query\": msg}, config={\"configurable\": {\"session_id\": \"id_k2\", \"k\": 2}}\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569cada",
   "metadata": {},
   "source": [
    "# 3. Message History + Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: OllamaLLM = Field(default_factory=OllamaLLM)\n",
    "\n",
    "    def __init__(self, llm: OllamaLLM):\n",
    "        super().__init__(llm=llm)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    \"Given the existing conversation summary and the new messages, \"\n",
    "                    \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                    \"as much relevant information as possible.\"\n",
    "                ),\n",
    "                HumanMessagePromptTemplate.from_template(\n",
    "                    \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                    \"New messages:\\n{messages}\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.messages, messages=messages\n",
    "            )\n",
    "        )\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "\n",
    "def get_chat_history(\n",
    "    session_id: str, llm: OllamaLLM\n",
    ") -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=OllamaLLM,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Invoke\n",
    "msgs = [\n",
    "    \"Hi, my name is James\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\",\n",
    "]\n",
    "for i, msg in enumerate(msgs):\n",
    "    print(f\"---\\nMessage {i + 1}\\n---\\n\")\n",
    "    response = pipeline_with_history.invoke(\n",
    "        {\"query\": msg}, config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    "    )\n",
    "    print(response)\n",
    "    print(\"\\nMessages in history:\")\n",
    "    print(chat_map[\"id_123\"].messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ae0aa",
   "metadata": {},
   "source": [
    "# 4. Message History + Window + Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: OllamaLLM = Field(default_factory=OllamaLLM)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: OllamaLLM, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages and summarizing the messages that we\n",
    "        drop.\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "\n",
    "        # see if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "        # check if we have too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"oldest {len(self.messages) - self.k} messages.\"\n",
    "            )\n",
    "            # pull out the oldest messages...\n",
    "            old_messages = self.messages[: self.k]\n",
    "            # ...and keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k :]\n",
    "\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            # if we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    \"Given the existing conversation summary and the new messages, \"\n",
    "                    \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                    \"as much relevant information as possible.\"\n",
    "                ),\n",
    "                HumanMessagePromptTemplate.from_template(\n",
    "                    \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                    \"New messages:\\n{old_messages}\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary, old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "        print(f\">> New summary: {new_summary}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b3a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "\n",
    "def get_chat_history(\n",
    "    session_id: str, llm: OllamaLLM, k: int\n",
    ") -> ConversationSummaryBufferMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce46ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=OllamaLLM,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Invoke\n",
    "msgs = [\n",
    "    \"Hi, my name is James\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\",\n",
    "]\n",
    "for i, msg in enumerate(msgs):\n",
    "    print(f\"---\\nMessage {i + 1}\\n---\\n\")\n",
    "    response = pipeline_with_history.invoke(\n",
    "        {\"query\": msg}, config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2299d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
