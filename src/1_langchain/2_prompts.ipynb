{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7fd123",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"deepseek-r1:1.5b\", reasoning=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d056de",
   "metadata": {},
   "source": [
    "# 1. Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"], template=\"Tell me a joke about {topic}.\"\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896591d",
   "metadata": {},
   "source": [
    "# 2. Chat: From template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deede0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Create a ChatPromptTemplate using a template string\n",
    "prompt_template = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "\n",
    "print(\"----- Prompt -----\")\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n----- Response -----\")\n",
    "result = llm.invoke(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbf6cb",
   "metadata": {},
   "source": [
    "# 3. Chat: From messsages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Prompt with System and Human Messages (Using Tuples)\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "print(\"----- Prompt -----\")\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n----- Response -----\")\n",
    "result = llm.invoke(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0109d",
   "metadata": {},
   "source": [
    "# 4. Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the template\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# We define a list of examples with dictionaries containing the correct input and output keys\n",
    "examples = [\n",
    "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
    "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
    "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"},\n",
    "]\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "# Here is the formatted prompt\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3e73c",
   "metadata": {},
   "source": [
    "# 5. Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77937f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question, you must:\n",
    "\n",
    "- List systematically and in precise detail all\n",
    "  subproblems that need to be solved to answer the\n",
    "  question.\n",
    "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
    "- Finally, use everything you have worked through to\n",
    "  provide the final answer.\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", cot_system_prompt),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cot_pipeline = cot_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
    "\n",
    "result = cot_pipeline.invoke({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03110bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
